## Experimental Setup
We divide the experiment in two parts: deployment and validation. We first  deploy the servers that run Abaco, it's components, and the compute nodes  that  Abaco  will  utilize  to  spawn  new  workers  on. Secondly,  we  run the test suite to conduct the performance studies.  Our entire testing repository, along with Docker images is hosted on Github, `TACC/abaco-autoscaling` with READMEs detailing the exact instructions to reproduce the experiment. In this section we describe the experimental process so the users can implement this method in their existing workflows

### Resource Configuration
All servers are hosted by TACC's self-service cloud system named Jetstream. This service allows for creation and configuration of interactive virtual machines (VMs), nodes, which gives our experiment the ability to scale with the addition of more nodes. To gain access to these resources an XSEDE account and allocation must first be obtained. Jetstream uses Openstack for node management and gives us the ability to deploy servers through the command line interface (CLI). This paper will continue to reference the resources used, but the Abaco platform is able to be ran on any cluster of Linux nodes at your disposal.

All nodes are running version `7.6.1810 of CentOS Linux`, `kernel version of 3.10.0-957.5.1.el7.x86_64`, `OpenStack version of 3.16.2`, `Docker version of 18.09.5`, and `Docker Compose version of 1.24.0`. All Abaco nodes are OpenStack m1.quad nodes which is made up of 10 GBs of RAM, 20 GBs of SSD storage, and 4 VCPUs. A VCPU in this case is one core of a Intel Xeon E5-2680 v3 which turbo-boosts to 3.30GHz from 2.50GHz.

### Resource Deployment
We start the deployment by creating an Openstack node. To do this we use the OpenStack horizon website located at tacc.jetstream-cloud.org. Once logged into this site we have to create one node which we refer to as 'perf'. The OpenStack image for this node is `perf-abaco-perf` if running through an XSEDE allocation. This gives you access to our image with all dependencies installed. You must also, with OpenStack, create a network for all nodes to communicate on and make the perf node public so that you can SSH into it remotely.

This is where the `TACC/abaco-autoscaling` Github repository comes in handy. In the /deployment folder of the repository there are scripts which handle deploying Abaco and all stages of managing compute nodes along with a README.md which provides an in-depth description of all of the scripts available. For the purposes of this paper we will focus solely on how to deploy instances and run the experiment.

Scripts that make use of the OpenStack CLI require OpenStack authentication. Thus, in the root directory of the repository is a script named `openrc-script` which will prompt for your password when run with `./openrc-script`. This scripts sets environment variables that will authenticate you with OpenStack once configured with a user's OpenStack URL, project ID, project name, and user domain name. 

%% Talking about deploying Abaco infrastructure.
Abaco deployment is the next step in our experiment setup. Abaco is capable of being completely hosted from one node, or even on the perf node, however Abaco was hosted by five separate nodes in our experiment for the sake of resource distribution and debugging purposes. All five of these nodes were OpenStack m1.quad servers that used the source image of 'perf-abaco-base'. This image contains all the Abaco docker-compose.yml files necessary to deploy across five nodes. To do this for yourself, in the repository, in the /deployment folder is a script named `init\_abaco` that when run prompts for the node name and then creates a m1.quad node with the input name with the `perf-abaco-base` image necessary. This image brings with it all the docker-compose.yml files needed to deploy Abaco.

These nodes can be initialized with following two commands. First, start the Docker with `sudo service docker start`. Second, to initialize several docker images, 'services', at once in order to create a system with correct networking and configuration using `docker-compose up -f "docker-compose-NODE\_WANTED.yml" -d`. There is a specific order to execute  'docker-compose up' on the nodes to configure the correct IP addresses of the databases.

First we create the MongoDB node using the `./init_abaco` script. Next, we login to the node by SSHing into it's private IP. On a successful login we start Docker, and 'docker-compose up' the `docker-compose-mdb.yml` file. This node is used for large document holding. Second the Redis node must be created, do the same, but this time the file is named `docker-compose-red.yml`. This node stores Abaco's system hierarchy. Now the third node to be brought up is the RabbitMQ node with file `docker-compose-rmq.yml`. This allows for queues of messages from the internet. The last two nodes can be brought up in either order, but the abaco.conf file inside the nodes must first be configured properly with the private IP address of the RabbitMQ, Redis, and Mongo nodes under the '[store]' and '[rabbit]' categories. These two nodes should also have the fourth section of their private IP inputted as the host\_id key in the '[spawner]' category of the `abaco.conf`. These two nodes will be the `docker-compose-prom.yml` and `docker-compose-web.yml` files. The first is not a necessary component, but allows for autoscaling when the `abaco.conf` file has the autoscaling key under '[workers]' set to 'True'. The second is the web interface that users receive and send messages to and is required to communicate between Abaco containers. Once all Abaco systems are deployed we have usability, however we first must create compute nodes in order for Abaco to have place to deploy requested work to.

%% Talking about compute deployment
Creation and management of the compute nodes has all been automated with scripts in the repository's /deployment folder. So in order to create a specified amount of compute nodes one would run `.up_instances` which will prompt the user how many nodes to deploy. This script will then create the specified amount of nodes through the OpenStack CLI and name them 'compute-\#' where the pound sign is the node index. These nodes will be initialized as OpenStack 'm1.medium' nodes with the OpenStack `perf-abaco-compute` image which is exactly like the `perf-abaco-perf` image, except that our testing images have already been cloned in order to get rid of time waiting for docker image downloading.

The next stage of compute deployment is running `docker-compose up -d` on all of these nodes which initializes an Abaco compute container. This task has been automated through the use of an Ansible playbook which reads in all servers in the OpenStack cluster and executes code if they are in the 'compute node' group. This playbook is run with `./up_abaco` which copies the `abaco.conf` and `docker-compose-compute.yml` file from the repositories `/abaco_files` directory to be utilized on each node.

The next two scripts are `burn\_dbs` and `down\_instances`. The first uses an Ansible playbook to go into all database nodes, delete all stored data, and reset all Docker containers. The second deletes all nodes named with compute in the node name and is used when testing completes.

In the case that you are attempting to run this experiment or even just Abaco on a personal or third-party Linux cluster you will not have use of the scripts made for this experiment or node images. To run this experiment you will need to deploy Abaco to several separate nodes or to one node by way of docker-compose. Followed by that, separate nodes must be created to be used as compute nodes for Abaco if you are not planning to have the Abaco compute containers run alongside the Abaco platform. Whether or not you use your own compute cluster or JetStream, you can still run the rest of the performance study assuming you have Abaco platform and at least one compute container running in the same node or in a separate node.

To finish deployment of the Abaco platform and of Abaco compute nodes you need to create the number of compute nodes that you want. In our case our highest node size is 89, so we run our `./up_instances` script and specify 89 nodes. Once the nodes are running and in an active state we start up Abaco compute containers on all of them with the `./up_abaco script`. This gets our compute nodes ready to begin our test.

### Validation Setup
The test suite is a highly automated process and requires little intervention or setup as it communicates solely with the already setup Abaco platform. The files for the suite are stored in the repository's `/test_suite` folder. Here you will find a folder named `/tests` that includes all the Python tests. To run these test scripts we have another script named `/run_tests.sh` which runs the test for all specified node sizes, each time deleting nodes in order to reach the specified amount of nodes wanted and re-initializing all containers.

To run the tests with the autoscaler on you need to uncomment the Python scripts inside the `/run\_tests.sh` script that begin with 'scaling'. You will also need to redeploy the Prometheus node initialized during deployment, but this time specifying the autoscaling key as 'True' in the `abaco.conf` file inside the node. Along with that, to specify the max workers per node for the autoscaler to follow you must set the `max_workers_per_host` key in the '[spawner]' section of the `abaco.conf` on the Prometheus node. Lastly, if you want to specify the maximum amount of workers per actor you will have to edit the `abaco.conf` file located in the repository's `/deployment/abaco_files` folder. Here you will change the `max_workers_per_actor` key under the '[spawner]' section. You must then redeploy the compute nodes by either deleting all nodes and redeploying or restarting Abaco on all by using the `down_abaco` script followed by the `up_abaco` script.

Each Python script is set with our difficulties and trial sizes. If you want to change the difficulty of the FLOPS tests you would change the 'SIZE' variable on line 18 of all FLOPS tests Python scripts. To change the difficulty of the hashrate test you would chance the 'HASHES' variable on line 12 of all hash tests Python scripts. Finally to change the amount of trials to do at each node size you would change the `num_runs` variable set in the 'main' function of all the scripts.

To run everything you simply have to run `./run_tests.sh` with the scripts and node sizes you want configured. The testing will then begin and data will be saved to .xlsx files in the repositories `/test_suite/data` and organized by test type.


### Test Execution
The execution of our experiment suite has five trials of six jobs at ten different node sizes. The first of the main three jobs is our easy FLOPS test. The test is setup with six Abaco actors per node to have one actor per node core. Each of these actors were given 5 executions to complete and each of these executions consisted of doing the dot product of two square matrices of dimensions 8000 by 8000. The second test is our hash test and is setup similarly to easy FLOPS except for the fact that each actor is given 6 executions and each execution is meant to complete 3,000,000 SHA256 hashes. The third test is our hard FLOPS test, this test has one Abaco actor per node so that each actor gets 6 cores to run on. Each of these actors is given 5 executions to complete and each of these executions consists of doing the dot product of two square matrices of dimensions 25,000 by 25,000. The last three tests are exactly the same as the first three, but they utilize the Abaco autoscaler for worker management.

|                 |  max_workers_per_host | max_workers_per_actor |
|-----------------|-----------------------|-----------------------|
| Hard FLOPs Test |           1           |           1           |
| Easy FLOPs Test |           6           |           30          |
| Hashrate Test   |           6           |           36          |

In our test we use the configurations listed in the above table for each test type. These parameters are set in the 	`abaco.conf` file in the Abaco files folder in the deployment folder of the repository and the Prometheus Docker container. These parameters allow us to ensure that we don't ruinously scale up our workers and cause bottlenecks in CPU distribution.

